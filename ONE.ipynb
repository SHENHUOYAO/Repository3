{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#模块导入区域\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, RandomizedSearchCV, cross_val_score, StratifiedKFold, cross_val_predict\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix,\n",
    "    recall_score, roc_auc_score, average_precision_score, precision_score, roc_curve\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier, SGDClassifier\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, AdaBoostClassifier, BaggingClassifier,\n",
    "    ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import optuna\n",
    "from collections import defaultdict\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.neural_network import MLPClassifier"
   ],
   "id": "bb2fc4e343058d5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "800bcaf72ddb15b9",
   "metadata": {},
   "source": [
    "matplotlib.use('TkAgg')  # 更改后端为TkAgg\n",
    "\n",
    "# 设置Seaborn风格\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------\n",
    "# 读取CSV文件 这个是数据集Y\n",
    "file_path = r'AZ31_ML.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "X = data.drop(columns=['Twinned'])\n",
    "y = data['Twinned'].astype(int)  # 将标签转为整数类型\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # 读取Excel文件 这个是数据集T\n",
    "# data = pd.read_excel(r'grain.xlsx')\n",
    "#\n",
    "# # 提取特征数据和目标数据\n",
    "# X = data.iloc[:, 1:19]\n",
    "# y = data.iloc[:, 19]\n",
    "\n",
    "# 使用SimpleImputer填补缺失值------------------------------------------------------------------------------------------------------------------\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# 2. Min-Max归一化（缩放到0-1区间）\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# 全部数据都作为训练集\n",
    "X_train = X_scaled\n",
    "y_train = y\n",
    "X_test = None\n",
    "y_test = None\n",
    "\n",
    "X_train = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "y_train = pd.Series(y, name='Lable')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 机器学习算法定义区域\n",
    "\n",
    "# 1. Logistic Regression\n",
    "param_space_lr = {\n",
    "    'C': lambda trial: trial.suggest_float('C', 1e-4, 1e4, log=True),\n",
    "    'penalty': lambda trial: trial.suggest_categorical('penalty', ['l2', None, 'l1', 'elasticnet']),  # 'none' 改为 None\n",
    "    'solver': lambda trial: trial.suggest_categorical('solver', ['lbfgs', 'saga', 'newton-cg', 'liblinear', 'sag']),\n",
    "    'max_iter': lambda trial: trial.suggest_int('max_iter', 200, 2000),\n",
    "    'class_weight': lambda trial: trial.suggest_categorical('class_weight', [None, 'balanced']),\n",
    "    'random_state': lambda trial: trial.suggest_categorical('random_state', [42]),             #123，2024非常离谱\n",
    "    'dual': lambda trial: trial.suggest_categorical('dual', [False, True]),\n",
    "    'tol': lambda trial: trial.suggest_float('tol', 1e-8, 1e-1, log=True),\n",
    "    'fit_intercept': lambda trial: trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "    'intercept_scaling': lambda trial: trial.suggest_float('intercept_scaling', 0.01, 100.0, log=True),\n",
    "    'multi_class': lambda trial: trial.suggest_categorical('multi_class', ['auto', 'ovr', 'multinomial']),\n",
    "    'verbose': lambda trial: trial.suggest_categorical('verbose', [0, 1, 2, 3, 4, 5]),\n",
    "    'warm_start': lambda trial: trial.suggest_categorical('warm_start', [False, True]),\n",
    "    'n_jobs': lambda trial: trial.suggest_categorical('n_jobs', [-1]),\n",
    "    # l1_ratio 只在 penalty=elasticnet 时添加\n",
    "}\n",
    "\n",
    "def objective_lr(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_lr.items()}\n",
    "    if params.get('penalty') == 'elasticnet':\n",
    "        params['l1_ratio'] = trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "    try:\n",
    "        model = LogisticRegression(**params)\n",
    "        score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "        return 0.0 if np.isnan(score) else score\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# 2. SVM 半新半旧\n",
    "param_space_svm = {\n",
    "    'C': lambda trial: trial.suggest_float('C', 1e-4, 1e4, log=True),  # 正则化参数，宽范围覆盖\n",
    "    'kernel': lambda trial: trial.suggest_categorical('kernel', ['linear', 'rbf', 'sigmoid', 'poly']),  # 添加 poly 核，增加模型灵活性\n",
    "    'gamma': lambda trial: trial.suggest_categorical('gamma', ['scale', 'auto', 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0]),  # 扩展 gamma，包含数值，适合 150 特征\n",
    "    'class_weight': lambda trial: trial.suggest_categorical('class_weight', [None, 'balanced']),  # 适配 recall_macro\n",
    "    'random_state': lambda trial: 42,  # 固定为 42，减少搜索复杂度\n",
    "    'tol': lambda trial: trial.suggest_float('tol', 1e-5, 1e-2, log=True),  # 添加收敛容差，优化性能\n",
    "    'coef0': lambda trial: trial.suggest_float('coef0', -1.0, 1.0) if trial.suggest_categorical('kernel', ['linear', 'rbf', 'sigmoid', 'poly']) in ['sigmoid', 'poly'] else 0.0,  # 添加，支持 sigmoid/poly 核\n",
    "    'degree': lambda trial: trial.suggest_int('degree', 2, 4) if trial.suggest_categorical('kernel', ['linear', 'rbf', 'sigmoid', 'poly']) == 'poly' else 3,  # 添加，支持 poly 核\n",
    "    'probability': lambda trial: True,  # 添加，确保与 train_and_evaluate 一致\n",
    "    'max_iter': lambda trial: trial.suggest_int('max_iter', 500, 5000),  # 控制最大迭代次数\n",
    "}\n",
    "\n",
    "def objective_svm(trial):   #添加了异常保护\n",
    "    try:  # 异常保护开始\n",
    "        params = {k: v(trial) for k, v in param_space_svm.items()}\n",
    "        # 处理无效参数组合\n",
    "        if params['kernel'] == 'linear':\n",
    "            params.pop('gamma', None)  # 线性核无需 gamma\n",
    "            params['coef0'] = 0.0\n",
    "            params['degree'] = 3\n",
    "        if params['kernel'] == 'rbf':\n",
    "            params['coef0'] = 0.0\n",
    "            params['degree'] = 3\n",
    "        params['cache_size'] = 500\n",
    "        model = SVC(**params)\n",
    "        score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "        return 0.0 if np.isnan(score) else score\n",
    "    except Exception as e:  # 异常保护：出错直接返回0.0\n",
    "        return 0.0\n",
    "\n",
    "# 3. Random Forest 旧的\n",
    "param_space_rf = {\n",
    "    'n_estimators': lambda trial: trial.suggest_int('n_estimators', 50, 500),\n",
    "    'max_depth': lambda trial: trial.suggest_int('max_depth', 3, 20),\n",
    "    'min_samples_split': lambda trial: trial.suggest_int('min_samples_split', 2, 20),\n",
    "    'min_samples_leaf': lambda trial: trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "    'max_features': lambda trial: trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "    'bootstrap': lambda trial: trial.suggest_categorical('bootstrap', [True, False]),\n",
    "    'class_weight': lambda trial: trial.suggest_categorical('class_weight', [None, 'balanced']),\n",
    "    'random_state': lambda trial: trial.suggest_categorical('random_state', [42, 123, 2024]),\n",
    "}\n",
    "def objective_rf(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_rf.items()}\n",
    "    model = RandomForestClassifier(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 4. KNN  新的\n",
    "param_space_knn = {\n",
    "    'n_neighbors': lambda trial: trial.suggest_int('n_neighbors', 1, 100),  # 扩展范围到100，适应1000样本的不同邻居规模\n",
    "    'weights': lambda trial: trial.suggest_categorical('weights', ['uniform', 'distance']),  # 保留，核心参数，控制邻居权重\n",
    "    'metric': lambda trial: trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'minkowski']),  # 保留，常用距离度量\n",
    "    'p': lambda trial: trial.suggest_int('p', 1, 5),  # 保留，minkowski核的p值\n",
    "    'algorithm': lambda trial: trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']),  # 添加，选择最佳搜索算法，适应150特征\n",
    "    'leaf_size': lambda trial: trial.suggest_int('leaf_size', 10, 50),  # 添加，优化树结构效率，范围适中\n",
    "}\n",
    "\n",
    "def objective_knn(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_knn.items()}\n",
    "    # 当metric不是minkowski时，忽略p\n",
    "    if params['metric'] != 'minkowski':\n",
    "        params.pop('p', None)\n",
    "    model = KNeighborsClassifier(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 5. Naive Bayes  新的\n",
    "param_space_nb = {\n",
    "    'var_smoothing': lambda trial: trial.suggest_float('var_smoothing', 1e-12, 1e-3, log=True),  # 扩展范围到1e-3，适应1000样本和150特征的方差平滑\n",
    "    'priors': lambda trial: trial.suggest_categorical('priors', [None, [0.5, 0.5], [0.3, 0.7], [0.7, 0.3]]),  # 添加，先验概率，适配二分类和recall_macro\n",
    "}\n",
    "\n",
    "def objective_nb(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_nb.items()}\n",
    "    model = GaussianNB(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 6. Decision Tree 新的\n",
    "param_space_dt = {\n",
    "    'max_depth': lambda trial: trial.suggest_categorical('max_depth', [None, 3, 5, 10, 15, 20, 30]),  # Extend to include None for deeper trees\n",
    "    'min_samples_split': lambda trial: trial.suggest_int('min_samples_split', 2, 50),  # Widen range for more flexibility\n",
    "    'min_samples_leaf': lambda trial: trial.suggest_int('min_samples_leaf', 1, 20),  # Widen range to prevent overfitting\n",
    "    'max_features': lambda trial: trial.suggest_float('max_features', 0.1, 1.0),  # Replace categorical with float for finer control over 150 features\n",
    "    'class_weight': lambda trial: trial.suggest_categorical('class_weight', [None, 'balanced']),  # Keep for recall_macro\n",
    "    'random_state': lambda trial: 42,  # Fix to single value to reduce search complexity\n",
    "    'criterion': lambda trial: trial.suggest_categorical('criterion', ['gini', 'entropy']),  # Add to explore different split criteria\n",
    "    'ccp_alpha': lambda trial: trial.suggest_float('ccp_alpha', 1e-6, 0.1, log=True)\n",
    "}\n",
    "\n",
    "def objective_dt(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_dt.items()}\n",
    "    model = DecisionTreeClassifier(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 7. Gradient Boosting\n",
    "param_space_gb = {\n",
    "    'n_estimators': lambda trial: trial.suggest_int('n_estimators', 50, 1000),  # Extend range for more trees, leveraging Optuna\n",
    "    'learning_rate': lambda trial: trial.suggest_float('learning_rate', 1e-4, 1.0, log=True),  # Keep wide range for flexibility\n",
    "    'max_depth': lambda trial: trial.suggest_int('max_depth', 3, 15),  # Extend to 15 for deeper trees\n",
    "    'min_samples_split': lambda trial: trial.suggest_int('min_samples_split', 2, 50),  # Widen range for flexibility\n",
    "    'min_samples_leaf': lambda trial: trial.suggest_int('min_samples_leaf', 1, 20),  # Widen range to prevent overfitting\n",
    "    'subsample': lambda trial: trial.suggest_float('subsample', 0.1, 1.0),  # Extend to 0.1 for more stochasticity\n",
    "    'random_state': lambda trial: 42,  # Fix to single value to reduce search complexity\n",
    "    'criterion': lambda trial: trial.suggest_categorical('criterion', ['friedman_mse', 'squared_error']),  # Add to explore split criteria\n",
    "    'max_features': lambda trial: trial.suggest_float('max_features', 0.1, 1.0),  # Add for feature subsampling, suits 150 features\n",
    "      'ccp_alpha': lambda trial: trial.suggest_float('ccp_alpha', 1e-6, 0.1, log=True)\n",
    "}\n",
    "\n",
    "def objective_gb(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_gb.items()}\n",
    "    model = GradientBoostingClassifier(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 8. XGBoost\n",
    "param_space_xgb = {\n",
    "    'n_estimators': lambda trial: trial.suggest_int('n_estimators', 50, 1000),  # Extend range for more trees\n",
    "    'learning_rate': lambda trial: trial.suggest_float('learning_rate', 1e-4, 1.0, log=True),  # Keep wide range\n",
    "    'max_depth': lambda trial: trial.suggest_int('max_depth', 3, 15),  # Extend to 15 for deeper trees\n",
    "    'min_child_weight': lambda trial: trial.suggest_int('min_child_weight', 1, 20),  # Widen range for robustness\n",
    "    'subsample': lambda trial: trial.suggest_float('subsample', 0.1, 1.0),  # Extend to 0.1 for more stochasticity\n",
    "    'colsample_bytree': lambda trial: trial.suggest_float('colsample_bytree', 0.1, 1.0),  # Extend to 0.1 for more feature subsampling\n",
    "    'gamma': lambda trial: trial.suggest_float('gamma', 0.0, 10.0),  # Widen range for more flexibility\n",
    "    'random_state': lambda trial: 42,  # Fix to single value\n",
    "    'reg_alpha': lambda trial: trial.suggest_float('reg_alpha', 1e-5, 10.0, log=True),  # Add L1 regularization for sparsity\n",
    "    'reg_lambda': lambda trial: trial.suggest_float('reg_lambda', 1e-5, 10.0, log=True),  # Add L2 regularization for robustness\n",
    "    'scale_pos_weight': lambda trial: trial.suggest_float('scale_pos_weight', 0.1, 10.0, log=True),  # Add for recall_macro and class imbalance\n",
    "}\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_xgb.items()}\n",
    "    model = xgb.XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss')\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 9. MLP\n",
    "param_space_mlp = {\n",
    "    'hidden_layer_sizes': lambda trial: trial.suggest_categorical('hidden_layer_sizes', [(50,), (100,), (150,), (50, 50), (100, 50), (100, 100)]),  # Add more architectures for 150 features\n",
    "    'activation': lambda trial: trial.suggest_categorical('activation', ['relu', 'tanh', 'logistic']),  # Keep for flexibility\n",
    "    'solver': lambda trial: trial.suggest_categorical('solver', ['adam', 'sgd']),  # Keep for optimization methods\n",
    "    'alpha': lambda trial: trial.suggest_float('alpha', 1e-5, 1e-1, log=True),  # Keep for regularization\n",
    "    'learning_rate': lambda trial: trial.suggest_categorical('learning_rate', ['constant', 'adaptive']),  # Keep for learning rate schedule\n",
    "    'max_iter': lambda trial: trial.suggest_int('max_iter', 200, 5000),  # Extend for convergence on 1000 samples\n",
    "    'random_state': lambda trial: 42,  # Fix to single value\n",
    "    'batch_size': lambda trial: trial.suggest_int('batch_size', 16, 256),  # Add for mini-batch size, suits 1000 samples\n",
    "    'learning_rate_init': lambda trial: trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True),  # Add for initial learning rate\n",
    "}\n",
    "\n",
    "def objective_mlp(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_mlp.items()}\n",
    "    model = MLPClassifier(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 10. LightGBM\n",
    "param_space_lgbm = {\n",
    "    'n_estimators': lambda trial: trial.suggest_int('n_estimators', 50, 250),  # Extend range for more trees\n",
    "    'learning_rate': lambda trial: trial.suggest_float('learning_rate', 1e-4, 1.0, log=True),  # Keep wide range\n",
    "    'max_depth': lambda trial: trial.suggest_int('max_depth', 3, 10),  # Extend to 15 for deeper trees\n",
    "    'num_leaves': lambda trial: trial.suggest_int('num_leaves', 20, 32),  # Widen range for more complex trees  这个太大会卡\n",
    "    'min_child_samples': lambda trial: trial.suggest_int('min_child_samples', 10, 50),  # Widen range for robustness\n",
    "    'subsample': lambda trial: trial.suggest_float('subsample', 0.1, 1.0),  # Extend to 0.1 for stochasticity\n",
    "    'colsample_bytree': lambda trial: trial.suggest_float('colsample_bytree', 0.1, 1.0),  # Extend to 0.1 for feature subsampling\n",
    "    'random_state': lambda trial: 42,  # Fix to single value\n",
    "    'reg_alpha': lambda trial: trial.suggest_float('reg_alpha', 1e-5, 10.0, log=True),  # Add L1 regularization\n",
    "    'reg_lambda': lambda trial: trial.suggest_float('reg_lambda', 1e-5, 10.0, log=True),  # Add L2 regularization\n",
    "    'class_weight': lambda trial: trial.suggest_categorical('class_weight', [None, 'balanced']),  # Add for recall_macro and class imbalance\n",
    "}\n",
    "\n",
    "import concurrent.futures\n",
    "import traceback\n",
    "\n",
    "def objective_lgbm(trial):\n",
    "    def run_trial():\n",
    "        params = {k: v(trial) for k, v in param_space_lgbm.items()}\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "        return 0.0 if np.isnan(score) else score\n",
    "\n",
    "    try:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            future = executor.submit(run_trial)\n",
    "            return future.result(timeout=30)\n",
    "    except concurrent.futures.TimeoutError:\n",
    "        print(f\"[WARN] Trial timeout! Trial number: {trial.number}\")\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Trial exception at trial {trial.number}: {e}\")\n",
    "        # 可选：traceback.print_exc()\n",
    "        return 0.0\n",
    "\n",
    "# 11. CatBoost  简化了\n",
    "param_space_cb = {\n",
    "    'iterations': lambda trial: trial.suggest_categorical('iterations', [100, 200, 300]),\n",
    "    'learning_rate': lambda trial: trial.suggest_categorical('learning_rate', [0.01, 0.1, 0.2]),\n",
    "    'depth': lambda trial: trial.suggest_categorical('depth', [3, 5, 10]),\n",
    "    'l2_leaf_reg': lambda trial: trial.suggest_categorical('l2_leaf_reg', [1, 3, 5]),\n",
    "    'random_state': lambda trial: 42,  # 固定随机种子\n",
    "}\n",
    "def objective_cb(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_cb.items()}\n",
    "    model = cb.CatBoostClassifier(**params, verbose=0)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 12. AdaBoost\n",
    "param_space_ab = {\n",
    "    'n_estimators': lambda trial: trial.suggest_int('n_estimators', 50, 1000),  # Extend range for more boosting\n",
    "    'learning_rate': lambda trial: trial.suggest_float('learning_rate', 1e-4, 1.0, log=True),  # Keep wide range\n",
    "    'random_state': lambda trial: 42,  # Fix to single value\n",
    "    'algorithm': lambda trial: trial.suggest_categorical('algorithm', ['SAMME'])\n",
    "}\n",
    "\n",
    "def objective_ab(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_ab.items()}\n",
    "    model = AdaBoostClassifier(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 13. Bagging Classifier\n",
    "param_space_bag = {\n",
    "    'n_estimators': lambda trial: trial.suggest_int('n_estimators', 10, 200),  # Extend range for more base models\n",
    "    'max_samples': lambda trial: trial.suggest_float('max_samples', 0.1, 1.0),  # Extend to 0.1 for more stochasticity\n",
    "    'max_features': lambda trial: trial.suggest_float('max_features', 0.1, 1.0),  # Extend to 0.1 for finer control\n",
    "    'bootstrap': lambda trial: trial.suggest_categorical('bootstrap', [True, False]),  # Keep for diversity\n",
    "    'random_state': lambda trial: 42,  # Fix to single value\n",
    "    'bootstrap_features': lambda trial: trial.suggest_categorical('bootstrap_features', [True, False]),  # Add for feature sampling diversity\n",
    "}\n",
    "\n",
    "def objective_bag(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_bag.items()}\n",
    "    model = BaggingClassifier(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 14. Extra Trees Classifier\n",
    "param_space_et = {\n",
    "    'n_estimators': lambda trial: trial.suggest_int('n_estimators', 50, 1000),  # Extend range for more trees\n",
    "    'max_depth': lambda trial: trial.suggest_categorical('max_depth', [None, 3, 5, 10, 15, 20, 30]),  # Include None for deeper trees\n",
    "    'min_samples_split': lambda trial: trial.suggest_int('min_samples_split', 2, 50),  # Widen range\n",
    "    'min_samples_leaf': lambda trial: trial.suggest_int('min_samples_leaf', 1, 20),  # Widen range\n",
    "    'max_features': lambda trial: trial.suggest_float('max_features', 0.1, 1.0),  # Replace categorical with float for finer control\n",
    "    'bootstrap': lambda trial: trial.suggest_categorical('bootstrap', [True, False]),  # Keep for diversity\n",
    "    'random_state': lambda trial: 42,  # Fix to single value\n",
    "    'criterion': lambda trial: trial.suggest_categorical('criterion', ['gini', 'entropy']),  # Add for split criteria\n",
    "    'ccp_alpha': lambda trial: trial.suggest_float('ccp_alpha', 1e-6, 0.1, log=True),  # Add for pruning\n",
    "    'class_weight': lambda trial: trial.suggest_categorical('class_weight', [None, 'balanced']),  # Add for recall_macro\n",
    "}\n",
    "\n",
    "def objective_et(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_et.items()}\n",
    "    model = ExtraTreesClassifier(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 15. Passive Aggressive Classifier\n",
    "param_space_pa = {\n",
    "    'C': lambda trial: trial.suggest_float('C', 1e-4, 1e4, log=True),  # Keep wide range for regularization\n",
    "    'max_iter': lambda trial: trial.suggest_int('max_iter', 200, 5000),  # Extend for convergence\n",
    "    'tol': lambda trial: trial.suggest_float('tol', 1e-8, 1e-1, log=True),  # Keep for convergence control\n",
    "    'loss': lambda trial: trial.suggest_categorical('loss', ['hinge', 'squared_hinge']),  # Keep for loss functions\n",
    "    'random_state': lambda trial: 42,  # Fix to single value\n",
    "    'class_weight': lambda trial: trial.suggest_categorical('class_weight', [None, 'balanced']),  # Add for recall_macro\n",
    "    'fit_intercept': lambda trial: trial.suggest_categorical('fit_intercept', [True, False]),  # Add to explore intercept\n",
    "}\n",
    "\n",
    "def objective_pa(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_pa.items()}\n",
    "    model = PassiveAggressiveClassifier(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 16. Ridge Classifier\n",
    "param_space_ridge = {\n",
    "    'alpha': lambda trial: trial.suggest_float('alpha', 1e-4, 1e4, log=True),  # Keep wide range for regularization\n",
    "    'max_iter': lambda trial: trial.suggest_int('max_iter', 200, 5000),  # Extend for convergence\n",
    "    'tol': lambda trial: trial.suggest_float('tol', 1e-8, 1e-1, log=True),  # Keep for convergence control\n",
    "    'class_weight': lambda trial: trial.suggest_categorical('class_weight', [None, 'balanced']),  # Keep for recall_macro\n",
    "    'random_state': lambda trial: 42,  # Fix to single value\n",
    "    'fit_intercept': lambda trial: trial.suggest_categorical('fit_intercept', [True, False]),  # Add to explore intercept\n",
    "    'solver': lambda trial: trial.suggest_categorical('solver', ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']),  # Add to explore solvers\n",
    "}\n",
    "\n",
    "def objective_ridge(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_ridge.items()}\n",
    "    model = RidgeClassifier(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 17. Linear Discriminant Analysis  这个需要先把数据里面的高度相关的东西清理掉\n",
    "param_space_lda = {\n",
    "    'solver': lambda trial: trial.suggest_categorical('solver', ['svd', 'lsqr', 'eigen']),\n",
    "    'tol': lambda trial: trial.suggest_float('tol', 1e-8, 1e-1, log=True),\n",
    "    'priors': lambda trial: trial.suggest_categorical('priors', [None, [0.5, 0.5], [0.3, 0.7], [0.7, 0.3]]),\n",
    "    # shrinkage 不直接在这里设置\n",
    "}\n",
    "\n",
    "def objective_lda(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_lda.items()}\n",
    "    # 只有 lsqr/eigen 支持 shrinkage\n",
    "    if params['solver'] in ['lsqr', 'eigen']:\n",
    "        params['shrinkage'] = trial.suggest_categorical('shrinkage', ['auto', 0.0, 0.5, 1.0])\n",
    "    model = LinearDiscriminantAnalysis(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 18. Quadratic Discriminant Analysis\n",
    "param_space_qda = {\n",
    "    'reg_param': lambda trial: trial.suggest_float('reg_param', 0.0, 1.0),  # Keep for regularization\n",
    "    'tol': lambda trial: trial.suggest_float('tol', 1e-8, 1e-1, log=True),  # Keep for convergence\n",
    "    'priors': lambda trial: trial.suggest_categorical('priors', [None, [0.5, 0.5], [0.3, 0.7], [0.7, 0.3]]),  # Add for class priors, suits recall_macro\n",
    "}\n",
    "\n",
    "def objective_qda(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_qda.items()}\n",
    "    model = QuadraticDiscriminantAnalysis(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 19. Stochastic Gradient Descent\n",
    "param_space_sgd = {\n",
    "    'loss': lambda trial: trial.suggest_categorical('loss', ['hinge', 'log_loss', 'modified_huber']),  # Update to include 'log_loss' instead of 'log'\n",
    "    'penalty': lambda trial: trial.suggest_categorical('penalty', ['l2', 'l1', 'elasticnet']),  # Keep for regularization\n",
    "    'alpha': lambda trial: trial.suggest_float('alpha', 1e-6, 1e-1, log=True),  # Keep for regularization strength\n",
    "    'max_iter': lambda trial: trial.suggest_int('max_iter', 200, 5000),  # Extend for convergence\n",
    "    'tol': lambda trial: trial.suggest_float('tol', 1e-8, 1e-1, log=True),  # Keep for convergence\n",
    "    'random_state': lambda trial: 42,  # Fix to single value\n",
    "    'l1_ratio': lambda trial: trial.suggest_float('l1_ratio', 0.0, 1.0),  # Add for elasticnet mixing\n",
    "    'learning_rate': lambda trial: trial.suggest_categorical('learning_rate', ['constant', 'optimal', 'invscaling', 'adaptive']),  # Add more schedules\n",
    "    'eta0': lambda trial: trial.suggest_float('eta0', 1e-4, 1.0, log=True),  # Add for initial learning rate\n",
    "    'class_weight': lambda trial: trial.suggest_categorical('class_weight', [None, 'balanced']),  # Add for recall_macro\n",
    "}\n",
    "\n",
    "def objective_sgd(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_sgd.items()}\n",
    "    model = SGDClassifier(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score\n",
    "\n",
    "# 20. Voting Classifier\n",
    "param_space_voting = {\n",
    "    'voting': lambda trial: trial.suggest_categorical('voting', ['hard', 'soft']),\n",
    "}\n",
    "def objective_voting(trial):\n",
    "    params = {k: v(trial) for k, v in param_space_voting.items()}\n",
    "    # Define base estimators (using default parameters for simplicity)\n",
    "    estimators = [\n",
    "        ('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svm', SVC(probability=True, random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('dt', DecisionTreeClassifier())\n",
    "    ]\n",
    "    model = VotingClassifier(estimators=estimators, **params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10, scoring='recall_macro', n_jobs=-1).mean()\n",
    "    return 0.0 if np.isnan(score) else score"
   ],
   "id": "90e79ac3aba86234",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 机器学习算法与对应目标函数的映射\n",
    "ALGORITHM_OBJECTIVES = {\n",
    "    'LogisticRegression': objective_lr,\n",
    "    'SVM': objective_svm,\n",
    "    'RandomForest': objective_rf,\n",
    "    'KNN': objective_knn,\n",
    "    'NaiveBayes': objective_nb,\n",
    "    'DecisionTree': objective_dt,\n",
    "    'GradientBoosting': objective_gb,\n",
    "    'XGBoost': objective_xgb,\n",
    "    'MLP': objective_mlp,\n",
    "    'LightGBM': objective_lgbm,\n",
    "    'CatBoost': objective_cb,\n",
    "    'AdaBoost': objective_ab,\n",
    "    'Bagging': objective_bag,\n",
    "    'ExtraTrees': objective_et,\n",
    "    'PassiveAggressive': objective_pa,\n",
    "    'Ridge': objective_ridge,\n",
    "    'LDA': objective_lda,\n",
    "    'QDA': objective_qda,\n",
    "    'SGD': objective_sgd,\n",
    "    'Voting': objective_voting\n",
    "}\n",
    "\n",
    "# 模型类映射，用于实例化模型\n",
    "MODEL_CLASSES = {\n",
    "    'LogisticRegression': LogisticRegression,\n",
    "    'SVM': SVC,\n",
    "    'RandomForest': RandomForestClassifier,\n",
    "    'KNN': KNeighborsClassifier,\n",
    "    'NaiveBayes': GaussianNB,\n",
    "    'DecisionTree': DecisionTreeClassifier,\n",
    "    'GradientBoosting': GradientBoostingClassifier,\n",
    "    'XGBoost': xgb.XGBClassifier,\n",
    "    'MLP': MLPClassifier,\n",
    "    'LightGBM': lgb.LGBMClassifier,\n",
    "    'CatBoost': cb.CatBoostClassifier,\n",
    "    'AdaBoost': AdaBoostClassifier,\n",
    "    'Bagging': BaggingClassifier,\n",
    "    'ExtraTrees': ExtraTreesClassifier,\n",
    "    'PassiveAggressive': PassiveAggressiveClassifier,\n",
    "    'Ridge': RidgeClassifier,\n",
    "    'LDA': LinearDiscriminantAnalysis,\n",
    "    'QDA': QuadraticDiscriminantAnalysis,\n",
    "    'SGD': SGDClassifier,\n",
    "    'Voting': VotingClassifier\n",
    "}\n",
    "\n",
    "def train_and_evaluate(\n",
    "    algorithms='all',  # 'all' 或指定算法名称列表，如 ['LogisticRegression', 'RandomForest']\n",
    "    scoring='recall_macro',  # 优化目标\n",
    "    random_trials=200,  # 随机搜索步数\n",
    "    tpe_trials=200,  # TPE 搜索步数\n",
    "    cv=10,  # 交叉验证折数\n",
    "    output_metrics=['recall', 'auc', 'auprc', 'recall0', 'recall1', 'precision0', 'precision1', 'accuracy', 'f1']  # 输出指标\n",
    "):\n",
    "    \"\"\"\n",
    "    训练并评估指定的机器学习算法，使用 Optuna 进行超参数优化，所有指标基于交叉验证的验证集。\n",
    "\n",
    "    参数:\n",
    "        algorithms: str 或 list，'all' 表示训练所有算法，或指定算法名称列表\n",
    "        scoring: str，优化目标，如 'recall_macro', 'accuracy' 等\n",
    "        random_trials: int，随机搜索的试验次数\n",
    "        tpe_trials: int，TPE 搜索的试验次数\n",
    "        cv: int，交叉验证折数\n",
    "        output_metrics: list，输出的评估指标\n",
    "\n",
    "    返回:\n",
    "        results: dict，包含每个算法的交叉验证评估指标、最优参数和训练好的模型（供后续 SHAP 分析）\n",
    "    \"\"\"\n",
    "    results = defaultdict(dict)\n",
    "\n",
    "    # 如果指定 'all'，获取所有算法，否则使用用户指定的算法列表\n",
    "    algo_list = list(ALGORITHM_OBJECTIVES.keys()) if algorithms == 'all' else algorithms\n",
    "\n",
    "    for algo_name in algo_list:\n",
    "        print(f\"Training {algo_name}...\")\n",
    "\n",
    "        # 获取目标函数\n",
    "        objective = ALGORITHM_OBJECTIVES.get(algo_name)\n",
    "        if objective is None:\n",
    "            print(f\"Objective function for {algo_name} not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # 创建 Optuna 学习\n",
    "        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.RandomSampler())\n",
    "\n",
    "        # 随机搜索\n",
    "        study.optimize(objective, n_trials=random_trials, n_jobs=-1)\n",
    "\n",
    "        # 切换到 TPE 采样器继续优化\n",
    "        study.sampler = optuna.samplers.TPESampler()\n",
    "        study.optimize(objective, n_trials=tpe_trials, n_jobs=-1)\n",
    "\n",
    "        # 获取最优参数\n",
    "        best_params = study.best_params\n",
    "        results[algo_name]['best_params'] = best_params\n",
    "        results[algo_name]['best_score'] = study.best_value\n",
    "\n",
    "        # 训练最终模型\n",
    "        model_class = MODEL_CLASSES.get(algo_name)\n",
    "        if model_class is None:\n",
    "            print(f\"Model class for {algo_name} not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # 特殊处理 VotingClassifier 和 SVM\n",
    "        if algo_name == 'Voting':\n",
    "            estimators = [\n",
    "                ('lr', LogisticRegression(random_state=42)),\n",
    "                ('rf', RandomForestClassifier(random_state=42)),\n",
    "                ('svm', SVC(probability=True, random_state=42))\n",
    "            ]\n",
    "            model = model_class(estimators=estimators, **best_params)\n",
    "        else:\n",
    "            # XGBoost 特殊参数\n",
    "            if algo_name == 'XGBoost':\n",
    "                best_params['use_label_encoder'] = False\n",
    "                best_params['eval_metric'] = 'logloss'\n",
    "            # CatBoost 关闭 verbose\n",
    "            if algo_name == 'CatBoost':\n",
    "                best_params['verbose'] = 0\n",
    "            # SVM 启用 probability=True\n",
    "            if algo_name == 'SVM':\n",
    "                best_params['probability'] = True\n",
    "            model = model_class(**best_params)\n",
    "\n",
    "        # 交叉验证评估\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "        results[algo_name][f'cv_{scoring}'] = cv_scores.mean()\n",
    "\n",
    "        # 自定义交叉验证循环以计算所有指标\n",
    "        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "        metric_scores = defaultdict(list)\n",
    "\n",
    "        for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "            X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "            if algo_name == 'Voting':\n",
    "                estimators = [\n",
    "                    ('lr', LogisticRegression(random_state=42)),\n",
    "                    ('rf', RandomForestClassifier(random_state=42)),\n",
    "                    ('svm', SVC(probability=True, random_state=42))\n",
    "                ]\n",
    "                fold_model = model_class(estimators=estimators, **best_params)\n",
    "            else:\n",
    "                fold_model = model_class(**best_params)\n",
    "\n",
    "            fold_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            # 预测验证集\n",
    "            y_pred = fold_model.predict(X_val_fold)\n",
    "            y_proba = fold_model.predict_proba(X_val_fold)[:, 1] if hasattr(fold_model, 'predict_proba') else None\n",
    "\n",
    "            # 计算指标\n",
    "            for metric in output_metrics:\n",
    "                if metric == 'recall':\n",
    "                    metric_scores['cv_recall_macro'].append(recall_score(y_val_fold, y_pred, average='macro'))\n",
    "                elif metric == 'auc' and y_proba is not None:\n",
    "                    metric_scores['cv_auc'].append(roc_auc_score(y_val_fold, y_proba))\n",
    "                elif metric == 'auprc' and y_proba is not None:\n",
    "                    metric_scores['cv_auprc'].append(average_precision_score(y_val_fold, y_proba))\n",
    "                elif metric == 'recall0':\n",
    "                    metric_scores['cv_recall0'].append(recall_score(y_val_fold, y_pred, pos_label=0))\n",
    "                elif metric == 'recall1':\n",
    "                    metric_scores['cv_recall1'].append(recall_score(y_val_fold, y_pred, pos_label=1))\n",
    "                elif metric == 'precision0':\n",
    "                    metric_scores['cv_precision0'].append(precision_score(y_val_fold, y_pred, pos_label=0))\n",
    "                elif metric == 'precision1':\n",
    "                    metric_scores['cv_precision1'].append(precision_score(y_val_fold, y_pred, pos_label=1))\n",
    "                elif metric == 'accuracy':\n",
    "                    metric_scores['cv_accuracy'].append(accuracy_score(y_val_fold, y_pred))\n",
    "                elif metric == 'f1':\n",
    "                    metric_scores['cv_f1_macro'].append(f1_score(y_val_fold, y_pred, average='macro'))\n",
    "\n",
    "        # 平均每折的指标\n",
    "        for metric_name, scores in metric_scores.items():\n",
    "            results[algo_name][metric_name] = np.mean(scores)\n",
    "\n",
    "        # 训练完整模型以存储（供 SHAP 分析）\n",
    "        model.fit(X_train, y_train)\n",
    "        results[algo_name]['model'] = model\n",
    "\n",
    "        # 验证 cv_recall_macro 是否等于 (cv_recall0 + cv_recall1) / 2\n",
    "        if 'cv_recall0' in results[algo_name] and 'cv_recall1' in results[algo_name]:\n",
    "            expected_recall_macro = (results[algo_name]['cv_recall0'] + results[algo_name]['cv_recall1']) / 2\n",
    "            if not np.isclose(results[algo_name]['cv_recall_macro'], expected_recall_macro, atol=1e-6):\n",
    "                print(f\"Warning: cv_recall_macro ({results[algo_name]['cv_recall_macro']}) does not match \"\n",
    "                      f\"(cv_recall0 + cv_recall1) / 2 ({expected_recall_macro}) for {algo_name}\")\n",
    "\n",
    "        print(f\"{algo_name} completed. Best {scoring}: {study.best_value:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "#指定指标 输出所有算法的得分\n",
    "def print_metric_for_all_algorithms(results, metric_name):\n",
    "    print(f\"\\n所有算法的 {metric_name} 得分：\")\n",
    "    for algo, metrics in results.items():\n",
    "        value = metrics.get(metric_name, None)\n",
    "        if value is not None:\n",
    "            print(f\"{algo}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{algo}: 无此指标\")"
   ],
   "id": "23dab4b410a98fd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 示例用法\n",
    "    results = train_and_evaluate(\n",
    "        algorithms=[\n",
    "            'LogisticRegression',\n",
    "                    'SVM',\n",
    "                    'RandomForest',\n",
    "                    'KNN',\n",
    "                    'NaiveBayes',\n",
    "                    'DecisionTree',\n",
    "                    'GradientBoosting',\n",
    "                    'XGBoost',\n",
    "                    'MLP',\n",
    "                    'LightGBM',   # 这个很卡要优化一下\n",
    "                    'AdaBoost',\n",
    "                    'Bagging',\n",
    "                    'ExtraTrees',\n",
    "                    'PassiveAggressive',\n",
    "                    'Ridge',\n",
    "                    'QDA',\n",
    "                    'SGD',\n",
    "                    'Voting'\n",
    "                    ],\n",
    "        scoring='roc_auc',            # precision_macro  recall_macro  roc_auc\n",
    "        random_trials=500,\n",
    "        tpe_trials=500,\n",
    "        cv=10,\n",
    "       # output_metrics=['recall', 'auc', 'auprc', 'recall0', 'recall1', 'precision0', 'precision1', 'accuracy']\n",
    "    )\n",
    "\n",
    "    # 输出所有每个算法的结果 依次输出一个算法的各种得分\n",
    "    for algo, metrics in results.items():\n",
    "        print(f\"\\nResults for {algo}:\")\n",
    "        for key, value in metrics.items():\n",
    "            if key != 'model':\n",
    "                print(f\"{key}: {value}\")\n"
   ],
   "id": "fd8d28a2349fdbfe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print_metric_for_all_algorithms(results, 'cv_recall_macro')      # 输出所有算法的 recall\n",
    "print_metric_for_all_algorithms(results, 'cv_auc')         # 输出所有算法的 auc\n",
    "print_metric_for_all_algorithms(results, 'cv_recall0')  # 输出所有算法的 precision1\n",
    "print_metric_for_all_algorithms(results, 'cv_recall1')\n",
    "print_metric_for_all_algorithms(results, 'cv_precision0')\n",
    "print_metric_for_all_algorithms(results, 'cv_precision1')\n",
    "print_metric_for_all_algorithms(results, 'cv_accuracy')\n",
    "print_metric_for_all_algorithms(results, 'cv_f1_macro')"
   ],
   "id": "f33953259bb09095",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 指定算法名称\n",
    "algo_name = 'LogisticRegression'  # 可改为'SVM'、'RandomForest'等   LogisticRegression XGBoost\n",
    "\n",
    "# 获取模型\n",
    "model = results[algo_name]['model']\n",
    "\n",
    "# 特征名\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# 选择SHAP解释器\n",
    "if algo_name in ['RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM', 'CatBoost', 'ExtraTrees']:\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "elif algo_name in ['LogisticRegression', 'Ridge', 'SGD']:\n",
    "    explainer = shap.LinearExplainer(model, X_train)\n",
    "else:\n",
    "    explainer = shap.KernelExplainer(model.predict_proba, X_train)\n",
    "\n",
    "# 计算SHAP值\n",
    "shap_values = explainer.shap_values(X_train)"
   ],
   "id": "841cff9334df8f44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# SHAP总结图\n",
    "if isinstance(shap_values, list):  # 二分类\n",
    "    shap.summary_plot(shap_values[1], X_train, feature_names=feature_names, max_display=200)\n",
    "else:\n",
    "    shap.summary_plot(shap_values, X_train, feature_names=feature_names, max_display=200)"
   ],
   "id": "8593ac303b9cd7fb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
